{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TF_KERAS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "\n",
    "import codecs\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import io\n",
    "import time\n",
    "\n",
    "# import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc0 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "# tf.enable_eager_execution()\n",
    "# tf.compat.v1.disable_eager_execution() #tf2 code\n",
    "\n",
    "print(tf.__version__, tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jpn-english material\n",
    "path_to_file = \"./jpn-eng/jpn.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert model parameters\n",
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 64\n",
    "OUTPUT_LAYER_NUM = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "PRETRAINED_PATH = 'bert-master\\\\multi_cased_L-12_H-768_A-12'\n",
    "CONFIG_PATH = os.path.join(PRETRAINED_PATH, 'bert_config.json')\n",
    "CHECKPOINT_PATH = os.path.join(PRETRAINED_PATH, 'bert_model.ckpt')\n",
    "VOCAB_PATH = os.path.join(PRETRAINED_PATH, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bert model\n",
    "bert_base = load_trained_model_from_checkpoint(\n",
    "  CONFIG_PATH,\n",
    "  CHECKPOINT_PATH,\n",
    "  training=False,\n",
    "  trainable=False,\n",
    "  output_layer_num=OUTPUT_LAYER_NUM,\n",
    "  seq_len=SEQ_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract = K.function([bert_base.layers[0].input, bert_base.layers[1].input], [bert_base.layers[-1].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare token->idx dictionary\n",
    "def make_token_dict(vocab_path):\n",
    "  token_dict = {}\n",
    "  with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "      if line != ' \\n':\n",
    "        token = line.strip()\n",
    "      else:\n",
    "        token = line.strip('\\n')\n",
    "      token_dict[token] = len(token_dict)\n",
    "  return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eng_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '[CLS] ' + w + ' [SEP]'\n",
    "    return w\n",
    "\n",
    "def preprocess_jpn_sentence(w):\n",
    "    m = MeCab.Tagger (\"-Owakati\")\n",
    "    w = '[CLS] ' + m.parse(w).strip().strip('\\n') + ' [SEP]'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    eng, jpn = [], []\n",
    "    for l in lines[:num_examples]:\n",
    "#         (eng_sentence, jpn_sentence) = l.split('\\t')\n",
    "        (eng_sentence, _) = l.split('\\t')\n",
    "        eng.append(preprocess_eng_sentence(eng_sentence))\n",
    "#         jpn.append(preprocess_jpn_sentence(jpn_sentence))\n",
    "    return eng, jpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng, jpn = create_dataset(path_to_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] hi . [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(eng[3])\n",
    "# print(jpn[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def tokenize(lang):\n",
    "    # convert words(of a sentence) into word indexes\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "                     filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    # padding word indexes(of sentences) to the same length(using maximum length of all sentences)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                           padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "# def load_dataset(path, num_examples=None):\n",
    "#     # creating cleaned input, output pairs\n",
    "#     targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "#     input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "#     target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "#     return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, input_vocab_path, num_examples=None, verbose=False):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, _ = create_dataset(path, num_examples)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  input_token_dict = make_token_dict(input_vocab_path)\n",
    "  inp_lang_tokenizer = Tokenizer(input_token_dict, cased=True)\n",
    "  \n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "  inp_ids, inp_segments = [], []\n",
    "  for l in lines[:num_examples]:\n",
    "      (_, jpn_sentence) = l.split('\\t')\n",
    "      id, segment = inp_lang_tokenizer.encode(jpn_sentence, max_len=SEQ_LEN)\n",
    "      inp_ids.append(id)\n",
    "      inp_segments.append(segment)\n",
    "      if verbose:\n",
    "        print('{}->{}{}'.format(jpn_sentence, id, segment))\n",
    "\n",
    "  input_tensor = []\n",
    "  for i, s in zip(inp_ids, inp_segments):\n",
    "    input_tensor.append([i,s])        \n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp_ids, inp_segments, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, VOCAB_PATH, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000 36000 9000 9000\n"
     ]
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 45000\n",
    "# input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, VOCAB_PATH, num_examples)\n",
    "\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "# max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), SEQ_LEN\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 64\n"
     ]
    }
   ],
   "source": [
    "print(max_length_targ, max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert(lang, tensor):\n",
    "#   if isinstance(lang, tf.keras.preprocessing.text.Tokenizer):\n",
    "#     for t in tensor:\n",
    "#       if t!=0:\n",
    "#         print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "#   else:\n",
    "#     print (lang.decode(tensor))\n",
    "\n",
    "# print (\"Input Language; index to word mapping\")\n",
    "# convert(inp_lang, input_tensor_train[0][0])\n",
    "# print ()\n",
    "# print (\"Target Language; index to word mapping\")\n",
    "# convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 3072 # concatenation of last 4 layer ouputs of Transformer\n",
    "units = 1024\n",
    "# vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_inp_size = len(inp_lang._token_dict)\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor_train = np.array(input_tensor_train)\n",
    "# target_tensor_train = np.array(target_tensor_train)\n",
    "\n",
    "encoder = Encoder(units, BATCH_SIZE)\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "example_input_batch = input_tensor_train[:64]\n",
    "\n",
    "sample_bert_output = bert_base.predict([example_input_batch[:, 0, :], example_input_batch[:, 1, :]], batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(input_tensor_train[:64].shape)\n",
    "# print(example_input_batch[:, 0, :].shape)\n",
    "# print(np.array(sample_bert_output).shape)\n",
    "# print(input_tensor_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 64, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "sample_output, sample_hidden = encoder(np.array(sample_bert_output), sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_with_bert_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt-45k-30epoch\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "#         prepared_inp = tf.concat([tf.expand_dims(inp[:, 0, :], 0), tf.expand_dims(inp[:, 1, :], 0)], 0)\n",
    "#         prepared_inp = tf.stack([inp[:, 0, :], inp[:, 1, :]])\n",
    "#         print(prepared_inp.shape)\n",
    "        bert_embedding = bert_base.predict([inp[:, 0, :], inp[:, 1, :]], batch_size=BATCH_SIZE)\n",
    "#         bert_embedding = bert_base.predict({'Input-Token':prepared_inp[0],'Input-Segment':prepared_inp[1]}, batch_size=BATCH_SIZE)\n",
    "#         bert_embedding = bert_base.predict([prepared_inp[0],prepared_inp[1]], batch_size=BATCH_SIZE)\n",
    "\n",
    "        \n",
    "#         print(bert_embedding.shape)        \n",
    "        \n",
    "        enc_output, enc_hidden = encoder(bert_embedding, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['[cls]']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(input_tensor_train), input_tensor_train.shape)\n",
    "# print(type(dataset), tf.compat.v1.data.get_output_types(dataset), tf.compat.v1.data.get_output_shapes(dataset))\n",
    "# iterator = dataset.make_one_shot_iterator()\n",
    "# next_x, next_y = iterator.get_next()\n",
    "# print(type(next_x), next_x.shape)\n",
    "# print(type(next_y), next_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepared_x = tf.stack([next_x[:, 0, :], next_x[:, 1, :]])\n",
    "# print(prepared_x.shape)\n",
    "# embedding_x = bert_base.predict([prepared_x[0],prepared_x[1]], batch_size=None, steps=1)\n",
    "# print(embedding_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_hidden = encoder.initialize_hidden_state()\n",
    "# enc_output, enc_hidden = encoder(embedding_x, enc_hidden)\n",
    "# dec_hidden = enc_hidden\n",
    "# dec_input = tf.expand_dims([targ_lang.word_index['[cls]']] * BATCH_SIZE, 1)\n",
    "# loss = 0\n",
    "# for t in range(1, next_y.shape[1]):\n",
    "#   # passing enc_output to the decoder\n",
    "#   predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "#   loss += loss_function(next_y[:, t], predictions)\n",
    "\n",
    "#   # using teacher forcing\n",
    "#   dec_input = tf.expand_dims(next_y[:, t], 1)\n",
    "# batch_loss = (loss / int(next_y.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.trainable_variables()\n",
    "# with tf.Session() as sess:\n",
    "#   tf.global_variables_initializer()\n",
    "#   enc_hidden = encoder.initialize_hidden_state()\n",
    "#   total_loss = 0\n",
    "#   while True:\n",
    "#     try:\n",
    "#       next_x, next_y = iterator.get_next()\n",
    "#       prepared_x = tf.stack([next_x[:, 0, :], next_x[:, 1, :]])\n",
    "#       bert_embedding = bert_base.predict([prepared_x[0],prepared_x[1]], batch_size=None, steps=1)\n",
    "#       enc_output, enc_hidden = encoder(bert_embedding, enc_hidden)\n",
    "#       dec_hidden = enc_hidden\n",
    "#       dec_input = tf.expand_dims([targ_lang.word_index['[cls]']] * BATCH_SIZE, 1)\n",
    "#       for t in range(1, next_y.shape[1]):\n",
    "#         # passing enc_output to the decoder\n",
    "#         predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "#         loss += loss_function(next_y[:, t], predictions)\n",
    "\n",
    "#         # using teacher forcing\n",
    "#         dec_input = tf.expand_dims(next_y[:, t], 1)\n",
    "#       batch_loss = (loss / int(next_y.shape[1]))\n",
    "#       sess.run(batch_loss)\n",
    "#     except tf.errors.OutOfRangeError:\n",
    "#       print(\"End of dataset\")  # ==> \"End of dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 4\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "\n",
    "#     start = time.time()\n",
    "\n",
    "#     enc_hidden = encoder.initialize_hidden_state()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     iterates = len(input_tensor_train) // BATCH_SIZE\n",
    "#     for batch in range(iterates):\n",
    "#         print(input_tensor_train[batch * 64:(batch+1) * 64].shape, type(input_tensor_train[batch * 64:(batch+1) * 64]))\n",
    "#         inp = input_tensor_train[batch * 64:(batch+1) * 64]\n",
    "#         targ = target_tensor_train[batch * 64:(batch+1) * 64]\n",
    "#         batch_loss = train_step(inp, targ, enc_hidden)\n",
    "#         total_loss += batch_loss        \n",
    "# #     for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "# #         batch_loss = train_step(inp, targ, enc_hidden)\n",
    "# #         total_loss += batch_loss\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "#                                                          batch,\n",
    "#                                                          batch_loss.eval(session=tf.Session())))\n",
    "#     # saving (checkpoint) the model every 2 epochs\n",
    "#     if (epoch + 1) % 2 == 0:\n",
    "#         checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "#     print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "#                                         total_loss / steps_per_epoch))\n",
    "#     print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "#         print(inp.shape, targ.shape)\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, verbose=False):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    if verbose == True:\n",
    "        print('Original sentence: {}'.format(sentence))\n",
    "\n",
    "    sentence = preprocess_jpn_sentence(sentence)\n",
    "    if verbose == True:\n",
    "        print('Preprocessed sentence: {}'.format(sentence))\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    if verbose == True:\n",
    "        print('Word sequences of sentence: {}'.format(inputs))\n",
    "        \n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    if verbose == True:\n",
    "        print('Paded sequences of sentence: {}'.format(inputs))\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    if verbose == True:\n",
    "        print('Tensor of sentence: {}'.format(inputs))\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    if verbose == True:\n",
    "        print('enc_out: {}'.format(enc_out))\n",
    "        print('enc_hidden: {}'.format(enc_hidden))\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "    if verbose == True:\n",
    "        print('dec_input: {}'.format(dec_input))\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        if verbose == True:\n",
    "            print('dec_input: {}'.format(dec_input))\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, verbose=False):\n",
    "    result, sentence, attention_plot = evaluate(sentence, verbose)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('雨が嫌い。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('いい天気ですね。', verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
