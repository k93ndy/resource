{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_KERAS=1\n"
     ]
    }
   ],
   "source": [
    "%env TF_KERAS=1\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import MeCab\n",
    "\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer, get_pretrained, PretrainedList, get_checkpoint_paths\n",
    "from matplotlib import rcParams\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "path_to_file = \"./jpn-eng/jpn.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bert model\n",
    "SEQ_LEN = 64\n",
    "OUTPUT_LAYER_NUM = 4\n",
    "TRANSFORMER_NUM = 12\n",
    "\n",
    "model_path = get_pretrained(PretrainedList.multi_cased_base)\n",
    "PRETRAINED_PATH = get_checkpoint_paths(model_path)\n",
    "CONFIG_PATH = PRETRAINED_PATH.config\n",
    "CHECKPOINT_PATH = PRETRAINED_PATH.checkpoint\n",
    "VOCAB_PATH = PRETRAINED_PATH.vocab\n",
    "\n",
    "bert_base = load_trained_model_from_checkpoint(\n",
    "  CONFIG_PATH,\n",
    "  CHECKPOINT_PATH,\n",
    "  training=False,\n",
    "  trainable=False,\n",
    "  output_layer_num=OUTPUT_LAYER_NUM,\n",
    "  seq_len=SEQ_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare token->idx dictionary\n",
    "def make_token_dict(vocab_path):\n",
    "  token_dict = {}\n",
    "  with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "      if line != ' \\n':\n",
    "        token = line.strip()\n",
    "      else:\n",
    "        token = line.strip('\\n')\n",
    "      token_dict[token] = len(token_dict)\n",
    "  return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eng_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '[CLS] ' + w + ' [SEP]'\n",
    "    return w\n",
    "\n",
    "def preprocess_jpn_sentence(w):\n",
    "    m = MeCab.Tagger (\"-Owakati\")\n",
    "    w = '[CLS] ' + m.parse(w).strip().strip('\\n') + ' [SEP]'\n",
    "    return w\n",
    "\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    eng, jpn = [], []\n",
    "    for l in lines[:num_examples]:\n",
    "#         (eng_sentence, jpn_sentence) = l.split('\\t')\n",
    "        (eng_sentence, _) = l.split('\\t')\n",
    "        eng.append(preprocess_eng_sentence(eng_sentence))\n",
    "#         jpn.append(preprocess_jpn_sentence(jpn_sentence))\n",
    "    return eng, jpn\n",
    "  \n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def tokenize(lang):\n",
    "    # convert words(of a sentence) into word indexes\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "                     filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    # padding word indexes(of sentences) to the same length(using maximum length of all sentences)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                           padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path, input_vocab_path, num_examples=None, verbose=False):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, _ = create_dataset(path, num_examples)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  input_token_dict = make_token_dict(input_vocab_path)\n",
    "  inp_lang_tokenizer = Tokenizer(input_token_dict, cased=True)\n",
    "  \n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "  inp_ids, inp_segments = [], []\n",
    "  for l in lines[:num_examples]:\n",
    "      (_, jpn_sentence) = l.split('\\t')\n",
    "      id, segment = inp_lang_tokenizer.encode(jpn_sentence, max_len=SEQ_LEN)\n",
    "      inp_ids.append(id)\n",
    "      inp_segments.append(segment)\n",
    "      if verbose:\n",
    "        print('{}->{}{}'.format(jpn_sentence, id, segment))\n",
    "\n",
    "  input_tensor = []\n",
    "  for i, s in zip(inp_ids, inp_segments):\n",
    "    input_tensor.append([i,s])        \n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\di.sun\\.keras\\datasets\\multi_cased_L-12_H-768_A-12\\vocab.txt\n",
      "36000 36000 9000 9000\n",
      "29 64\n"
     ]
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 45000\n",
    "# input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, VOCAB_PATH, num_examples)\n",
    "input_tensor = np.array(input_tensor)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "# max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), SEQ_LEN\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
    "print(max_length_targ, max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119547 8844\n"
     ]
    }
   ],
   "source": [
    "input_vocab_size = len(make_token_dict(VOCAB_PATH))\n",
    "target_vocab_size = len(targ_lang.word_index) + 1\n",
    "print(input_vocab_size, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_encoder_norm(model, output_layer_num, transformer_num):\n",
    "  output_layer_num = [-i for i in range(1, output_layer_num + 1)]\n",
    "  target_layers = []\n",
    "  for layer_index in output_layer_num:\n",
    "      if layer_index < 0:\n",
    "          layer_index = transformer_num + layer_index\n",
    "      layer_index += 1\n",
    "      layer = model.get_layer(name='Encoder-{}-FeedForward-Norm'.format(layer_index))\n",
    "      target_layers.append(layer.output)\n",
    "  output = keras.layers.add(target_layers)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = bert_base.inputs[:2]\n",
    "outputs = sum_of_encoder_norm(bert_base, OUTPUT_LAYER_NUM, TRANSFORMER_NUM)\n",
    "bert_summed = keras.models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = input_tensor_train.shape[0]\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = input_tensor_train.shape[0]//BATCH_SIZE\n",
    "# vocab_inp_size = len(inp_lang.word_index)+1\n",
    "# vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7708, shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7723, shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask, verbose=False):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  if verbose:\n",
    "    print('matmul_qk: {}'.format(matmul_qk))\n",
    "    \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "  if verbose:\n",
    "    print('scaled_attention_logits: {}'.format(scaled_attention_logits))\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v, verbose=False):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None, verbose)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul_qk: [[  0. 100.   0.   0.]]\n",
      "scaled_attention_logits: [[ 0.       57.735027  0.        0.      ]]\n",
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul_qk: [[  0.   0. 100. 100.]]\n",
      "scaled_attention_logits: [[ 0.        0.       57.735027 57.735027]]\n",
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul_qk: [[100. 100.   0.   0.]]\n",
      "scaled_attention_logits: [[57.735027 57.735027  0.        0.      ]]\n",
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul_qk: [[  0.   0. 100. 100.]\n",
      " [  0. 100.   0.   0.]\n",
      " [100. 100.   0.   0.]]\n",
      "scaled_attention_logits: [[ 0.        0.       57.735027 57.735027]\n",
      " [ 0.       57.735027  0.        0.      ]\n",
      " [57.735027 57.735027  0.        0.      ]]\n",
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# pass all the queries together\n",
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "#     self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "#     x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch = input_tensor_train[:64]\n",
    "sample_bert_output = bert_summed.predict([example_input_batch[:, 0, :], example_input_batch[:, 1, :]], batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 768)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_bert_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 768)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=768, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=input_vocab_size)\n",
    "\n",
    "sample_encoder_output = sample_encoder(sample_bert_output, \n",
    "                                       training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n",
    "               rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(target_vocab_size, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000)\n",
    "\n",
    "output, attn = sample_decoder(tf.random.uniform((64, 26)), \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False, look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 26, 8000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 62))\n",
    "temp_target = tf.random.uniform((64, 26))\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119547 8844\n"
     ]
    }
   ],
   "source": [
    "num_layers = 6 # 6 in the paper\n",
    "d_model = 768 # 512 in the paper\n",
    "dff = 2048 # 2048 in the paper\n",
    "num_heads = 8\n",
    "\n",
    "dropout_rate = 0.1\n",
    "\n",
    "print(input_vocab_size, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./transformer_checkpoints\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#   ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#   print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_tensor_train[:64, 0, :], target_tensor_train[:64, :-1])\n",
    "\n",
    "predictions, _ = transformer(bert_embedding, target_tensor_train[:64, :-1], \n",
    "                             True, \n",
    "                             enc_padding_mask, \n",
    "                             combined_mask, \n",
    "                             dec_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 28, 8844])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64), # care default length of integer\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "# @tf.function(input_signature=train_step_signature)\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  print(inp.shape, tar.shape)\n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp[:, 0, :], tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    bert_embedding = bert_summed.predict([inp[:, 0, :], inp[:, 1, :]], steps=1)\n",
    "    predictions, _ = transformer(bert_embedding, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 2, 64) (64, 29)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in converted code:\n\n    <ipython-input-101-f454119363dd>:23 train_step  *\n        bert_embedding = bert_summed.predict([inp[:, 0, :], inp[:, 1, :]], steps=1)\n    C:\\Users\\di.sun\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py:915 predict\n        use_multiprocessing=use_multiprocessing)\n    C:\\Users\\di.sun\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py:722 predict\n        callbacks=callbacks)\n    C:\\Users\\di.sun\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py:299 model_iteration\n        batch_outs = f(actual_inputs)\n    C:\\Users\\di.sun\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3633 __call__\n        [x._numpy() for x in outputs],  # pylint: disable=protected-access\n    C:\\Users\\di.sun\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3633 <listcomp>\n        [x._numpy() for x in outputs],  # pylint: disable=protected-access\n\n    AttributeError: 'Tensor' object has no attribute '_numpy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-0133f04feef8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[1;31m# inp -> portuguese, tar -> english\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[0minitializer_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    368\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    369\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 370\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2146\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2147\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2148\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2036\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2037\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2038\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2039\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2040\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    913\u001b[0m                                           converted_func)\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 905\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    906\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in converted code:\n\n    <ipython-input-101-f454119363dd>:23 train_step  *\n        bert_embedding = bert_summed.predict([inp[:, 0, :], inp[:, 1, :]], steps=1)\n    C:\\Users\\di.sun\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py:915 predict\n        use_multiprocessing=use_multiprocessing)\n    C:\\Users\\di.sun\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py:722 predict\n        callbacks=callbacks)\n    C:\\Users\\di.sun\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py:299 model_iteration\n        batch_outs = f(actual_inputs)\n    C:\\Users\\di.sun\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3633 __call__\n        [x._numpy() for x in outputs],  # pylint: disable=protected-access\n    C:\\Users\\di.sun\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3633 <listcomp>\n        [x._numpy() for x in outputs],  # pylint: disable=protected-access\n\n    AttributeError: 'Tensor' object has no attribute '_numpy'\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence, verbose=False):\n",
    "  inp_sentence = preprocess_jpn_sentence(inp_sentence)\n",
    "  \n",
    "  inputs = [inp_lang.word_index[i] for i in inp_sentence.split(' ')]\n",
    "  \n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "  \n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "#   encoder_input = tf.expand_dims(inputs, 0)\n",
    "  encoder_input = inputs\n",
    "  if verbose:\n",
    "    print('encoder_input: {} {}'.format(encoder_input, encoder_input.shape))\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "  decoder_input = [targ_lang.word_index['<start>']]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "  if verbose:\n",
    "    print('initial output: {} {}'.format(output, output.shape))\n",
    "    \n",
    "  for i in range(max_length_targ):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "    if verbose:\n",
    "      print(enc_padding_mask, '\\n', combined_mask, '\\n', dec_padding_mask)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if targ_lang.index_word[int(predicted_id)] == '<end>':\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "  fig = plt.figure(figsize=(48, 24))\n",
    "  \n",
    "  sentence = preprocess_jpn_sentence(sentence)\n",
    "  sentence = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "#   sentence = tokenizer_pt.encode(sentence)\n",
    "  \n",
    "  attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "  for head in range(attention.shape[0]):\n",
    "    ax = fig.add_subplot(8, 1, head+1)\n",
    "    \n",
    "    # plot the attention weights\n",
    "    ax.matshow(attention[head][:-1, :len(sentence)], cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 16}\n",
    "\n",
    "    ax.set_xticks(range(len(sentence)))\n",
    "    ax.set_yticks(range(len(result)))\n",
    "    \n",
    "    ax.set_ylim(len(result)-1.5, -0.5)\n",
    "        \n",
    "    ax.set_xticklabels(\n",
    "        [inp_lang.index_word[i] for i in sentence], \n",
    "        fontdict=fontdict, rotation=90)\n",
    "    \n",
    "    ax.set_yticklabels([targ_lang.index_word[i] for i in result], \n",
    "                       fontdict=fontdict)\n",
    "    \n",
    "    ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, plot='', verbose=False):\n",
    "  result, attention_weights = evaluate(sentence, verbose=verbose)\n",
    "  \n",
    "  result = result.numpy()\n",
    "  predicted_sentence = ''\n",
    "  for x in result:\n",
    "    predicted_sentence += targ_lang.index_word[x]\n",
    "    predicted_sentence += ' '\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "  if plot:\n",
    "    plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 外は雪ですね。\n",
      "Predicted translation: <start> it s snowing outside . \n"
     ]
    }
   ],
   "source": [
    "test = '外は雪ですね。'\n",
    "# translate(test, plot='decoder_layer6_block2', verbose=False)\n",
    "translate(test, plot='', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 助けが必要です。\n",
      "Predicted translation: <start> i need help . \n"
     ]
    }
   ],
   "source": [
    "translate('助けが必要です。', plot='', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: いい天気ですね。\n",
      "Predicted translation: <start> it s a nice day . \n"
     ]
    }
   ],
   "source": [
    "translate('いい天気ですね。', plot='', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('どうすればいいでしょうか？', plot='decoder_layer6_block2', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 釣りにいきませんか？\n",
      "Predicted translation: <start> don t you go fishing ? \n"
     ]
    }
   ],
   "source": [
    "translate('釣りにいきませんか？', plot='', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 教科書を見てください。\n",
      "Predicted translation: <start> please look at the right . \n"
     ]
    }
   ],
   "source": [
    "translate('教科書を見てください。', plot='', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.font_manager as fm\n",
    "# fm.findSystemFonts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13824\n",
      "13827 8846\n"
     ]
    }
   ],
   "source": [
    "print(len(inp_lang.word_index))\n",
    "print(input_vocab_size, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_dataset.batch(1).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(a):\n",
    "  for inp, targ in a:\n",
    "    return inp[0,0,:], targ[0,0,:]\n",
    "inp_test, targ_test = fetch(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[   1 7009    4   95   10 5589   20    3    2    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(44,), dtype=int32) tf.Tensor(\n",
      "[   1 4911   34   14  576 2783   92    3    2    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0], shape=(29,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(inp_test, targ_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(inp):\n",
    "\n",
    "  encoder_input = tf.expand_dims(inp, 0)\n",
    "  print('encoder_input: {} {}'.format(encoder_input, encoder_input.shape))\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "  decoder_input = [targ_lang.word_index['<start>']]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "  print('initial output: {} {}'.format(output, output.shape))\n",
    "    \n",
    "  for i in range(max_length_targ):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "    print(enc_padding_mask, '\\n', combined_mask, '\\n', dec_padding_mask)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if targ_lang.index_word[int(predicted_id)] == '<end>':\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = test_func(inp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1  77 255 208  49  40  15 350  13   8   9]\n",
      "<start>\n",
      "so\n",
      "since\n",
      "ever\n",
      "will\n",
      "be\n",
      ",\n",
      "aren\n",
      "t\n",
      "you\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "o = o[0].numpy()\n",
    "print(o)\n",
    "for i in o:\n",
    "  print(targ_lang.index_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
